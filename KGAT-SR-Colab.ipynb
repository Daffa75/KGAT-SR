{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KGAT-SR_for_Predictive_Maintenance.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# KGAT-SR for Predictive Maintenance\n",
        "\n",
        "This notebook adapts the **Knowledge-Enhanced Graph Attention Network for Session-based Recommendation (KGAT-SR)** model for a predictive maintenance task. The goal is to predict which vehicle parts might need replacement in the next maintenance session based on historical work orders.\n",
        "\n",
        "### How to Use This Notebook:\n",
        "1.  **Upload Data**: In the Colab file explorer on the left, create a folder. The folder name should match the `dataset` name in the \"Configuration\" cell below (e.g., `toyota_maintenance`). Upload your `train.txt`, `test.txt`, and `kg.txt` files into this folder.\n",
        "2.  **Configure Parameters**: Adjust the settings in the \"Configuration\" cell as needed for your specific dataset and desired model hyperparameters.\n",
        "3.  **Run All Cells**: Execute the cells sequentially from top to bottom. The training process will begin in the final cell."
      ],
      "metadata": {
        "id": "intro_cell"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup and Imports\n",
        "\n",
        "This cell installs the required libraries and imports all the necessary Python packages."
      ],
      "metadata": {
        "id": "setup_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "setup_code"
      },
      "outputs": [],
      "source": [
        "# Install necessary libraries\n",
        "!pip install networkx pandas numpy torch\n",
        "\n",
        "# Import all required packages\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.nn import Module, Parameter\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import networkx as nx\n",
        "\n",
        "import pickle\n",
        "import math\n",
        "import datetime\n",
        "import time\n",
        "import os\n",
        "import argparse"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Configuration\n",
        "\n",
        "All hyperparameters and settings are defined here. Instead of using `argparse` from a command line, we use an `argparse.Namespace` to make it easy to modify settings directly in the notebook."
      ],
      "metadata": {
        "id": "config_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "config_code"
      },
      "outputs": [],
      "source": [
        "# All hyperparameters and settings\n",
        "args = argparse.Namespace(\n",
        "    # --- Critical Settings ---\n",
        "    dataset='toyota_maintenance', # <<< CHANGE THIS to your dataset folder name\n",
        "    batchSize=100,\n",
        "    hiddenSize=100, # Also used for embedding size\n",
        "    epoch=30,\n",
        "    lr=0.001,\n",
        "    l2=1e-5,\n",
        "\n",
        "    # --- Model & Training Details ---\n",
        "    lr_dc=0.1,\n",
        "    lr_dc_step=3,\n",
        "    step=1, # GNN propagation steps\n",
        "    patience=10, # Epochs to wait for improvement before early stopping\n",
        "    nonhybrid=False, # If true, only uses global preference, not the last item's info\n",
        "    validation=False, # If true, splits train set for validation\n",
        "    valid_portion=0.1,\n",
        "\n",
        "    # --- KGAT-Specific Settings (For your implementation) ---\n",
        "    emb_size=100,\n",
        "    neibor_size=4, # Number of neighbors to sample from KG\n",
        "    attr_size=2, # Number of attributes to sample\n",
        "    aggregate='concat', # How to aggregate KG info: 'concat', 'sum', etc.\n",
        "    \n",
        "    # --- Other Settings ---\n",
        "    n_workers=2, # Dataloader workers. Use 2 for Colab.\n",
        ")\n",
        "\n",
        "# Set the device to CUDA (GPU) if available, otherwise CPU\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "print(\"Configuration:\", args)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Data Loading and Knowledge Graph Classes\n",
        "\n",
        "This section contains the classes and functions required to handle the knowledge graph (`KGraph`) and the session data (`Data`). The code is adapted from the original `kg.py` and `utils.py` files."
      ],
      "metadata": {
        "id": "data_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "data_code"
      },
      "outputs": [],
      "source": [
        "# Adapted from kg.py\n",
        "class KGraph():\n",
        "    def __init__(self, dataset, attr_size):\n",
        "        self.dataset = dataset\n",
        "        self.sample_attr_size = attr_size\n",
        "        self.G, self.n_relation = self.get_kg()\n",
        "        self.n_entity = self.G.number_of_nodes()\n",
        "        print(f\"Knowledge Graph loaded: {self.n_entity} entities, {self.n_relation} relations.\")\n",
        "\n",
        "    def get_kg(self):\n",
        "        kg_path = f'/content/{self.dataset}/kg.txt'\n",
        "        if not os.path.exists(kg_path):\n",
        "            raise FileNotFoundError(f\"Knowledge graph file not found at: {kg_path}\")\n",
        "        G = nx.Graph()\n",
        "        all_rels = set()\n",
        "        with open(kg_path, 'r') as f:\n",
        "            for line in f.readlines():\n",
        "                arr = line.strip().split('\\t')\n",
        "                if len(arr) != 3:\n",
        "                    continue\n",
        "                head, rel, tail = int(arr[0]), arr[1], int(arr[2])\n",
        "                G.add_edge(head, tail, rel=rel)\n",
        "                all_rels.add(rel)\n",
        "        return G, len(all_rels)\n",
        "\n",
        "# Adapted from utils.py\n",
        "def data_masks(all_usr_pois, item_tail):\n",
        "    us_lens = [len(upois) for upois in all_usr_pois]\n",
        "    len_max = max(us_lens)\n",
        "    us_pois = [upois + item_tail * (len_max - le) for upois, le in zip(all_usr_pois, us_lens)]\n",
        "    us_msks = [[1] * le + [0] * (len_max - le) for le in us_lens]\n",
        "    return us_pois, us_msks, len_max\n",
        "\n",
        "class Data():\n",
        "    def __init__(self, data, shuffle=False):\n",
        "        inputs = data[0]\n",
        "        inputs, mask, len_max = data_masks(inputs, [0])\n",
        "        self.inputs = np.asarray(inputs)\n",
        "        self.mask = np.asarray(mask)\n",
        "        self.len_max = len_max\n",
        "        self.targets = np.asarray(data[1])\n",
        "        self.length = len(inputs)\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def generate_batch(self, batch_size):\n",
        "        if self.shuffle:\n",
        "            shuffled_arg = np.arange(self.length)\n",
        "            np.random.shuffle(shuffled_arg)\n",
        "            self.inputs = self.inputs[shuffled_arg]\n",
        "            self.mask = self.mask[shuffled_arg]\n",
        "            self.targets = self.targets[shuffled_arg]\n",
        "        n_batch = int(self.length / batch_size)\n",
        "        if self.length % batch_size != 0:\n",
        "            n_batch += 1\n",
        "        slices = np.split(np.arange(n_batch * batch_size), n_batch)\n",
        "        slices[-1] = np.arange(self.length - batch_size, self.length)\n",
        "        return slices\n",
        "\n",
        "    def get_slice(self, i):\n",
        "        inputs, mask, targets = self.inputs[i], self.mask[i], self.targets[i]\n",
        "        items, n_node, A, alias_inputs = [], [], [], []\n",
        "        for u_input in inputs:\n",
        "            n_node.append(len(np.unique(u_input)))\n",
        "        max_n_node = np.max(n_node)\n",
        "        for u_input in inputs:\n",
        "            node = np.unique(u_input)\n",
        "            items.append(node.tolist() + (max_n_node - len(node)) * [0])\n",
        "            u_A = np.zeros((max_n_node, max_n_node))\n",
        "            for i in np.arange(len(u_input) - 1):\n",
        "                if u_input[i + 1] == 0:\n",
        "                    break\n",
        "                u = np.where(node == u_input[i])[0][0]\n",
        "                v = np.where(node == u_input[i + 1])[0][0]\n",
        "                u_A[u][v] = 1\n",
        "            u_sum_in = np.sum(u_A, 0)\n",
        "            u_sum_in[np.where(u_sum_in == 0)] = 1\n",
        "            u_A_in = np.divide(u_A, u_sum_in)\n",
        "            u_sum_out = np.sum(u_A, 1)\n",
        "            u_sum_out[np.where(u_sum_out == 0)] = 1\n",
        "            u_A_out = np.divide(u_A.transpose(), u_sum_out)\n",
        "            A.append(np.concatenate([u_A_in, u_A_out]).transpose())\n",
        "            alias_inputs.append([np.where(node == i)[0][0] for i in u_input])\n",
        "        return alias_inputs, A, items, mask, targets"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Model Definition\n",
        "\n",
        "This section contains the core model classes: `GNN` (the Gated Graph Neural Network cell) and `SRGAT` (the main model). The code is primarily adapted from `SRGATM.py` and `model.py`."
      ],
      "metadata": {
        "id": "model_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "model_code"
      },
      "outputs": [],
      "source": [
        "# GNN class from model.py and SRGATM.py\n",
        "class GNN(Module):\n",
        "    def __init__(self, hidden_size, step=1):\n",
        "        super(GNN, self).__init__()\n",
        "        self.step = step\n",
        "        self.hidden_size = hidden_size\n",
        "        self.input_size = hidden_size * 2\n",
        "        self.gate_size = 3 * hidden_size\n",
        "        self.w_ih = Parameter(torch.Tensor(self.gate_size, self.input_size))\n",
        "        self.w_hh = Parameter(torch.Tensor(self.gate_size, self.hidden_size))\n",
        "        self.b_ih = Parameter(torch.Tensor(self.gate_size))\n",
        "        self.b_hh = Parameter(torch.Tensor(self.gate_size))\n",
        "        self.b_iah = Parameter(torch.Tensor(self.hidden_size))\n",
        "        self.b_oah = Parameter(torch.Tensor(self.hidden_size))\n",
        "        self.linear_edge_in = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
        "        self.linear_edge_out = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
        "\n",
        "    def GNNCell(self, A, hidden):\n",
        "        input_in = torch.matmul(A[:, :, :A.shape[1]], self.linear_edge_in(hidden)) + self.b_iah\n",
        "        input_out = torch.matmul(A[:, :, A.shape[1]: 2 * A.shape[1]], self.linear_edge_out(hidden)) + self.b_oah\n",
        "        inputs = torch.cat([input_in, input_out], 2)\n",
        "        gi = F.linear(inputs, self.w_ih, self.b_ih)\n",
        "        gh = F.linear(hidden, self.w_hh, self.b_hh)\n",
        "        i_r, i_i, i_n = gi.chunk(3, 2)\n",
        "        h_r, h_i, h_n = gh.chunk(3, 2)\n",
        "        resetgate = torch.sigmoid(i_r + h_r)\n",
        "        inputgate = torch.sigmoid(i_i + h_i)\n",
        "        newgate = torch.tanh(i_n + resetgate * h_n)\n",
        "        hy = newgate + inputgate * (hidden - newgate)\n",
        "        return hy\n",
        "\n",
        "    def forward(self, A, hidden):\n",
        "        for i in range(self.step):\n",
        "            hidden = self.GNNCell(A, hidden)\n",
        "        return hidden\n",
        "\n",
        "# SRGAT class, merging SessionGraph and SRGAT logic\n",
        "class SRGAT(nn.Module):\n",
        "    def __init__(self, args, n_node, n_items, n_rels):\n",
        "        super().__init__()\n",
        "        self.hidden_size = args.hiddenSize\n",
        "        self.n_node = n_node\n",
        "        self.n_items = n_items\n",
        "        self.n_rels = n_rels\n",
        "        self.batch_size = args.batchSize\n",
        "        self.nonhybrid = args.nonhybrid\n",
        "\n",
        "        # Item and relation embeddings\n",
        "        self.embedding = nn.Embedding(self.n_node, self.hidden_size)\n",
        "        # You might need a relation embedding table for your KG logic\n",
        "        # self.relation_embedding = nn.Embedding(self.n_rels, self.hidden_size)\n",
        "\n",
        "        # GNN for session graph\n",
        "        self.gnn = GNN(self.hidden_size, step=args.step)\n",
        "\n",
        "        # Attention and output layers\n",
        "        self.linear_one = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
        "        self.linear_two = nn.Linear(self.hidden_size, self.hidden_size, bias=True)\n",
        "        self.linear_three = nn.Linear(self.hidden_size, 1, bias=False)\n",
        "        self.linear_transform = nn.Linear(self.hidden_size * 2, self.hidden_size, bias=True)\n",
        "\n",
        "        # Loss and optimizer\n",
        "        self.loss_function = nn.CrossEntropyLoss()\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "        self.scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=args.lr_dc_step, gamma=args.lr_dc)\n",
        "        \n",
        "        self.reset_parameters()\n",
        "\n",
        "    def reset_parameters(self):\n",
        "        stdv = 1.0 / math.sqrt(self.hidden_size)\n",
        "        for weight in self.parameters():\n",
        "            weight.data.uniform_(-stdv, stdv)\n",
        "\n",
        "    def compute_scores(self, hidden, mask):\n",
        "        # Get the last item's embedding\n",
        "        ht = hidden[torch.arange(mask.shape[0]).long(), torch.sum(mask, 1) - 1]\n",
        "        \n",
        "        # Attention mechanism to get session embedding\n",
        "        q1 = self.linear_one(ht).view(ht.shape[0], 1, ht.shape[1])\n",
        "        q2 = self.linear_two(hidden)\n",
        "        alpha = self.linear_three(torch.sigmoid(q1 + q2))\n",
        "        session_emb = torch.sum(alpha * hidden * mask.view(mask.shape[0], -1, 1).float(), 1)\n",
        "\n",
        "        # Combine session embedding with last item's embedding\n",
        "        if not self.nonhybrid:\n",
        "            session_emb = self.linear_transform(torch.cat([session_emb, ht], 1))\n",
        "        \n",
        "        # =====================================================================\n",
        "        # TODO: This is where you would integrate the knowledge graph embedding.\n",
        "        # For example, you could get a KG embedding for the session (`kg_emb`)\n",
        "        # and combine it with the `session_emb`.\n",
        "        # final_emb = session_emb + kg_emb \n",
        "        # =====================================================================\n",
        "        final_emb = session_emb\n",
        "\n",
        "        # Calculate scores against all items\n",
        "        b = self.embedding.weight[1:]  # n_nodes x latent_size\n",
        "        scores = torch.matmul(final_emb, b.transpose(1, 0))\n",
        "        return scores\n",
        "\n",
        "    def forward(self, items, A):\n",
        "        hidden = self.embedding(items)\n",
        "        hidden = self.gnn(A, hidden)\n",
        "        return hidden"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Training and Evaluation Logic\n",
        "\n",
        "These functions handle the core logic for a single training/testing step. They orchestrate the forward pass, loss calculation, backpropagation, and performance metric calculation."
      ],
      "metadata": {
        "id": "training_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "training_code"
      },
      "outputs": [],
      "source": [
        "# Utility functions to move tensors to the correct device\n",
        "def trans_to_cuda(variable):\n",
        "    return variable.to(device)\n",
        "\n",
        "def trans_to_cpu(variable):\n",
        "    return variable.to('cpu')\n",
        "\n",
        "# Main forward pass for a slice of data\n",
        "def model_forward_pass(model, i, data):\n",
        "    alias_inputs, A, items, mask, targets = data.get_slice(i)\n",
        "    alias_inputs = trans_to_cuda(torch.Tensor(alias_inputs).long())\n",
        "    items = trans_to_cuda(torch.Tensor(items).long())\n",
        "    A = trans_to_cuda(torch.Tensor(A).float())\n",
        "    mask = trans_to_cuda(torch.Tensor(mask).long())\n",
        "    \n",
        "    hidden = model(items, A)\n",
        "    \n",
        "    get_hidden = lambda i: hidden[i][alias_inputs[i]]\n",
        "    seq_hidden = torch.stack([get_hidden(i) for i in torch.arange(len(alias_inputs)).long()])\n",
        "    \n",
        "    return targets, model.compute_scores(seq_hidden, mask)\n",
        "\n",
        "# Main function to run a full epoch of training and evaluation\n",
        "def train_test(model, train_data, test_data):\n",
        "    model.scheduler.step()\n",
        "    print('start training: ', datetime.datetime.now())\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    slices = train_data.generate_batch(model.batch_size)\n",
        "    for i, j in zip(slices, np.arange(len(slices))):\n",
        "        model.optimizer.zero_grad()\n",
        "        targets, scores = model_forward_pass(model, i, train_data)\n",
        "        targets = trans_to_cuda(torch.Tensor(targets).long())\n",
        "        loss = model.loss_function(scores, targets - 1)\n",
        "        loss.backward()\n",
        "        model.optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "        if j % int(len(slices) / 5 + 1) == 0:\n",
        "            print('[%d/%d] Loss: %.4f' % (j, len(slices), loss.item()))\n",
        "    print('\\tTotal Loss:\\t%.3f' % total_loss)\n",
        "\n",
        "    print('start predicting: ', datetime.datetime.now())\n",
        "    model.eval()\n",
        "    hit, mrr = [], []\n",
        "    slices = test_data.generate_batch(model.batch_size)\n",
        "    with torch.no_grad():\n",
        "        for i in slices:\n",
        "            targets, scores = model_forward_pass(model, i, test_data)\n",
        "            sub_scores = scores.topk(20)[1]\n",
        "            sub_scores = trans_to_cpu(sub_scores).detach().numpy()\n",
        "            targets = trans_to_cpu(torch.Tensor(targets)).numpy()\n",
        "            for score, target, mask in zip(sub_scores, targets, test_data.mask[i]):\n",
        "                hit.append(np.isin(target - 1, score))\n",
        "                if len(np.where(score == target - 1)[0]) == 0:\n",
        "                    mrr.append(0)\n",
        "                else:\n",
        "                    mrr.append(1 / (np.where(score == target - 1)[0][0] + 1))\n",
        "    hit = np.mean(hit) * 100\n",
        "    mrr = np.mean(mrr) * 100\n",
        "    return hit, mrr"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Main Execution Block\n",
        "\n",
        "This is where everything comes together. After ensuring your data is uploaded correctly, running this cell will:\n",
        "1. Load the `train.txt` and `test.txt` data.\n",
        "2. Determine the number of unique items (`n_node`).\n",
        "3. Initialize the `SRGAT` model.\n",
        "4. Start the training and evaluation loop."
      ],
      "metadata": {
        "id": "main_markdown"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "main_code"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    print(\"--- KGAT-SR for Predictive Maintenance ---\")\n",
        "    \n",
        "    # --- Data Loading ---\n",
        "    train_data_path = f'/content/{args.dataset}/train.txt'\n",
        "    test_data_path = f'/content/{args.dataset}/test.txt'\n",
        "\n",
        "    if not os.path.exists(train_data_path) or not os.path.exists(test_data_path):\n",
        "        print(f\"\\nERROR: Data files not found in /content/{args.dataset}/\")\n",
        "        print(\"Please ensure the folder exists and contains train.txt and test.txt.\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nStep 1: Loading data from /content/{args.dataset}...\")\n",
        "    with open(train_data_path, 'rb') as f:\n",
        "        train_data_raw = pickle.load(f)\n",
        "    with open(test_data_path, 'rb') as f:\n",
        "        test_data_raw = pickle.load(f)\n",
        "    \n",
        "    # Determine the number of unique items (nodes)\n",
        "    all_seqs = train_data_raw[0] + test_data_raw[0]\n",
        "    # The +1 is because item IDs are 1-based, so max ID is the number of items\n",
        "    n_node = max(max(seq) for seq in all_seqs if seq) + 1 \n",
        "    print(f\"Data loaded. Found {n_node-1} unique items/parts.\")\n",
        "\n",
        "    train_data = Data(train_data_raw, shuffle=True)\n",
        "    test_data = Data(test_data_raw, shuffle=False)\n",
        "\n",
        "    # --- Model Initialization ---\n",
        "    print(\"\\nStep 2: Initializing model and knowledge graph...\")\n",
        "    kg = KGraph(args.dataset, args.attr_size)\n",
        "    \n",
        "    model = SRGAT(args, n_node, kg.n_entity, kg.n_relation)\n",
        "    model = trans_to_cuda(model)\n",
        "\n",
        "    # --- Training Loop ---\n",
        "    print(\"\\nStep 3: Starting training loop...\")\n",
        "    start = time.time()\n",
        "    best_result = [0, 0]\n",
        "    best_epoch = [0, 0]\n",
        "    bad_counter = 0\n",
        "\n",
        "    for epoch in range(args.epoch):\n",
        "        print('-------------------------------------------------------')\n",
        "        print('Epoch: ', epoch)\n",
        "        hit, mrr = train_test(model, train_data, test_data)\n",
        "        flag = 0\n",
        "        if hit >= best_result[0]:\n",
        "            best_result[0] = hit\n",
        "            best_epoch[0] = epoch\n",
        "            flag = 1\n",
        "        if mrr >= best_result[1]:\n",
        "            best_result[1] = mrr\n",
        "            best_epoch[1] = epoch\n",
        "            flag = 1\n",
        "        print('\\nResults for this epoch:')\n",
        "        print(f'\\tHR@20: {hit:.4f}\\tMRR@20: {mrr:.4f}')\n",
        "        print('\\nBest Result so far:')\n",
        "        print('\\tHR@20: %.4f\\tMRR@20: %.4f\\tEpochs: %d, %d' % (\n",
        "            best_result[0], best_result[1], best_epoch[0], best_epoch[1]))\n",
        "        bad_counter += 1 - flag\n",
        "        if bad_counter >= args.patience:\n",
        "            print(f\"\\nEarly stopping triggered after {args.patience} epochs with no improvement.\")\n",
        "            break\n",
        "\n",
        "    print('-------------------------------------------------------')\n",
        "    end = time.time()\n",
        "    print(\"Training finished.\")\n",
        "    print(\"Run time: %f s\" % (end - start))\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ]
    }
  ]
}
